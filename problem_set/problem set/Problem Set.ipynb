{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39965269-3870-4c9c-9f38-fe3c33d67c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c5dc07-3053-4703-9cbc-c287fb6a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cab5d-0f82-443f-bada-1e91731381c1",
   "metadata": {},
   "source": [
    "## 1. Implement a PyTorch model with Adam on FashionMnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca38572-77b1-4891-a6c4-c6d94925fabc",
   "metadata": {},
   "source": [
    "There are many optimizers that employ adaptive learning rates to account for the different learning rate needs at different phases of training. Your job is to first implement such an optimizer and see its performance. For now, we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449efaa-ffe0-4682-abe6-2ba574317c24",
   "metadata": {},
   "source": [
    "### a. Download Data FashionMnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306583f6-01b9-42e2-af1f-69d62b18f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "transform_method = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_method)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1af7f5-d3c8-4db9-8c22-861d867c4e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "for test_images, test_labels in train_loader:  \n",
    "    sample_image = test_images[0]\n",
    "    sample_label = test_labels[0]\n",
    "    plt.imshow(sample_image[0], cmap='gray')\n",
    "\"\"\"\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f009cae-65e5-42e2-8baa-6003a23628ba",
   "metadata": {},
   "source": [
    "### b. Implement a Logistic Regression Model and Fit the Data with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fee1e32-f406-4ab7-820b-03f086fe6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd2054-9210-49fb-a55a-8f4f5ad4edda",
   "metadata": {},
   "source": [
    "## 2. Implement Adam with HD, and comprare it with Vanilla Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad1ed6-55f2-4014-9af9-e56ab56cf4da",
   "metadata": {},
   "source": [
    "## 3. Compare MARTHE on Adam to AdamHD\n",
    "Implement MARTHE. Use the learning rate scheduling to tune the learning rate of Adam, and compare the results to Adam with HD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bcede52-a4aa-421f-a49a-5f33ff8ea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import sys  \n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    # Change the line below to include the directory you are currently in\n",
    "    sys.path.append(module_path+\"\\\\project\\\\problem set\")\n",
    "print(sys.path)\"\"\"\n",
    "\n",
    "from adatune.data_loader import *\n",
    "from adatune.mu_adam import MuAdam\n",
    "from adatune.mu_sgd import MuSGD\n",
    "from adatune.network import *\n",
    "from adatune.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e10e547-874d-4c39-b97b-7bd86d0d0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rtho(network_name, dataset, num_epoch, batch_size, optim_name, lr, momentum, wd, hyper_lr, alpha,\n",
    "               grad_clipping, first_order, seed, mu=1.0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # We are using cuda for training - no point trying out on CPU for ResNet\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    net = network(network_name, dataset)\n",
    "    net.to(device).apply(init_weights)\n",
    "\n",
    "    # assign argparse parameters\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    cur_lr = lr\n",
    "    timestep = 0\n",
    "\n",
    "    train_data, test_data = data_loader(network, dataset, batch_size)\n",
    "\n",
    "    if optim_name == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd, eps=1e-4)\n",
    "        hyper_optim = MuAdam(optimizer, hyper_lr, grad_clipping, first_order, mu, alpha, device)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "        hyper_optim = MuSGD(optimizer, hyper_lr, grad_clipping, first_order, mu, alpha, device)\n",
    "\n",
    "    vg = ValidationGradient(test_data, nn.CrossEntropyLoss(), device)\n",
    "    for epoch in range(num_epoch):\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, labels in train_data:\n",
    "            net.train()\n",
    "            timestep += 1\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_pred = outputs.argmax(1)\n",
    "            train_correct += train_pred.eq(labels).sum().item()\n",
    "\n",
    "            first_grad = ag.grad(loss, net.parameters(), create_graph=True, retain_graph=True)\n",
    "\n",
    "            hyper_optim.compute_hg(net, first_grad)\n",
    "\n",
    "            for params, gradients in zip(net.parameters(), first_grad):\n",
    "                params.grad = gradients\n",
    "\n",
    "            optimizer.step()\n",
    "            hyper_optim.hyper_step(vg.val_grad(net))\n",
    "            clear_grad(net)\n",
    "\n",
    "        train_acc = 100.0 * (train_correct / len(train_data.dataset))\n",
    "        val_loss, val_acc = compute_loss_accuracy(net, test_data, criterion, device)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "\n",
    "        print('train_accuracy at epoch :{} is : {}'.format(epoch, train_acc))\n",
    "        print('val_accuracy at epoch :{} is : {}'.format(epoch, val_acc))\n",
    "        print('best val_accuracy is : {}'.format(best_val_accuracy))\n",
    "\n",
    "        cur_lr = 0.0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            cur_lr = param_group['lr']\n",
    "        print('learning_rate after epoch :{} is : {}'.format(epoch, cur_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c8536a-afc0-4f31-aaa9-0715838080a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 914.10 MiB already allocated; 0 bytes free; 958.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-98bb1d616ae3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train_with_marthe(model3, 0.001, train_loader, test_loader, 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_rtho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"resnet\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cifar_10\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-e2d90e467af5>\u001b[0m in \u001b[0;36mtrain_rtho\u001b[1;34m(network_name, dataset, num_epoch, batch_size, optim_name, lr, momentum, wd, hyper_lr, alpha, grad_clipping, first_order, seed, mu)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mfirst_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mhyper_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_hg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\All\\Projects\\Hypergradient-Descent\\problem_set\\problem set\\adatune\\mu_adam.py\u001b[0m in \u001b[0;36mcompute_hg\u001b[1;34m(self, net, first_grad)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoeff\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm_t\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_t\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mhvp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_flatten\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhvp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mgrad_flatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_flatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 914.10 MiB already allocated; 0 bytes free; 958.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#train_with_marthe(model3, 0.001, train_loader, test_loader, 10)\n",
    "train_rtho(\"resnet\", \"cifar_10\", 10, 16, \"adam\", 0.0001, 0.9, 0, 0.0001, 1e-6, 100.0, False, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822479b-2324-4c10-b43a-519fdba2f8b2",
   "metadata": {},
   "source": [
    "## 4. Task for Graduate Students\n",
    "### a. Repeat task 1 and task 2 on VGG. Compare performance increase of AdamHD on the non-convex optimization problem when compared to Adam.\n",
    "Comment on your findings and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759fbe6-73fb-4ced-9173-0113b32d52d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
