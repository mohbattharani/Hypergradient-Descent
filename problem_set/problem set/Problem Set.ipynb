{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39965269-3870-4c9c-9f38-fe3c33d67c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c5dc07-3053-4703-9cbc-c287fb6a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cab5d-0f82-443f-bada-1e91731381c1",
   "metadata": {},
   "source": [
    "## 1. Implement a PyTorch model with VGG on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca38572-77b1-4891-a6c4-c6d94925fabc",
   "metadata": {},
   "source": [
    "There are many optimizers that employ adaptive learning rates to account for the different learning rate needs at different phases of training. Your job is to first implement such an optimizer and see its performance. For now, we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449efaa-ffe0-4682-abe6-2ba574317c24",
   "metadata": {},
   "source": [
    "### a. Download Data CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306583f6-01b9-42e2-af1f-69d62b18f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "transform_method = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize( \n",
    "       (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) \n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_method)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1af7f5-d3c8-4db9-8c22-861d867c4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f009cae-65e5-42e2-8baa-6003a23628ba",
   "metadata": {},
   "source": [
    "### b. Implement a VGG Model and Fit the Data with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee1e32-f406-4ab7-820b-03f086fe6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        if not (epoch+1) % 1 == 0:\n",
    "            return\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch+1, result['val_loss'], result['val_acc']))\n",
    "\n",
    "class CIFAR10VGGModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.vgg16().to(device)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    max_probabilities, predictions = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856c2a1-36a2-4b63-9e37-8a2d2ee3659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CIFAR10VGGModel().to(device)\n",
    "history1 = fit(20, 0.0005, model1, train_loader, test_loader, torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83533116-ff1f-468d-a5c5-8d10f2b2a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle(\"Learning Trend\")\n",
    "val_loss = [element[\"val_loss\"] for element in history1]\n",
    "val_accuracy = [element[\"val_acc\"] for element in history1]\n",
    "ax1.plot(val_loss, label=\"Validation Loss\")\n",
    "ax2.plot(val_accuracy, label=\"Validation Accuracy\")\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd2054-9210-49fb-a55a-8f4f5ad4edda",
   "metadata": {},
   "source": [
    "## 2. Implement SGD with Hypergradient Descent, and comprare it with SGD without Hypergradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe807f-44a3-4fc2-a96e-0498d2350714",
   "metadata": {},
   "source": [
    "### This is already done in our code, Using VGG with SGD on CIFAR10 data set. Please refer to our code base for solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad1ed6-55f2-4014-9af9-e56ab56cf4da",
   "metadata": {},
   "source": [
    "## 3. Compare MARTHE on SGD to SGD-HD.\n",
    "Implement MARTHE. Use the learning rate scheduling to tune the learning rate of SGD, and compare the results to SGD with HD on VGG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0223e-9c31-41ae-b6ee-bc64b16b0766",
   "metadata": {},
   "source": [
    "### Import MARTHE Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcede52-a4aa-421f-a49a-5f33ff8ea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adatune.data_loader import *\n",
    "from adatune.mu_adam import MuAdam\n",
    "from adatune.mu_sgd import MuSGD\n",
    "from adatune.network import *\n",
    "from adatune.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ecc68-5564-4660-b427-84418a131773",
   "metadata": {},
   "source": [
    "### Define Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10e547-874d-4c39-b97b-7bd86d0d0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rtho(network_name, dataset, num_epoch, batch_size, optim_name, lr, momentum, wd, hyper_lr, alpha,\n",
    "               grad_clipping, first_order, seed, mu=1.0):\n",
    "    torch.manual_seed(seed)\n",
    "    return\n",
    "\n",
    "    # We are using cuda for training - no point trying out on CPU for ResNet\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    net = network(network_name, dataset)\n",
    "    net.to(device).apply(init_weights)\n",
    "\n",
    "    # assign argparse parameters\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    cur_lr = lr\n",
    "    timestep = 0\n",
    "\n",
    "    train_data, test_data = data_loader(network, dataset, batch_size)\n",
    "\n",
    "    if optim_name == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd, eps=1e-4)\n",
    "        hyper_optim = MuAdam(optimizer, hyper_lr, grad_clipping, first_order, mu, alpha, device)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
    "        hyper_optim = MuSGD(optimizer, hyper_lr, grad_clipping, first_order, mu, alpha, device)\n",
    "\n",
    "    vg = ValidationGradient(test_data, nn.CrossEntropyLoss(), device)\n",
    "    for epoch in range(num_epoch):\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "\n",
    "        for inputs, labels in train_data:\n",
    "            net.train()\n",
    "            timestep += 1\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_pred = outputs.argmax(1)\n",
    "            train_correct += train_pred.eq(labels).sum().item()\n",
    "\n",
    "            first_grad = ag.grad(loss, net.parameters(), create_graph=True, retain_graph=True)\n",
    "\n",
    "            hyper_optim.compute_hg(net, first_grad)\n",
    "\n",
    "            for params, gradients in zip(net.parameters(), first_grad):\n",
    "                params.grad = gradients\n",
    "\n",
    "            optimizer.step()\n",
    "            hyper_optim.hyper_step(vg.val_grad(net))\n",
    "            clear_grad(net)\n",
    "\n",
    "        train_acc = 100.0 * (train_correct / len(train_data.dataset))\n",
    "        val_loss, val_acc = compute_loss_accuracy(net, test_data, criterion, device)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "\n",
    "        print('train_accuracy at epoch :{} is : {}'.format(epoch, train_acc))\n",
    "        print('val_accuracy at epoch :{} is : {}'.format(epoch, val_acc))\n",
    "        print('best val_accuracy is : {}'.format(best_val_accuracy))\n",
    "\n",
    "        cur_lr = 0.0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            cur_lr = param_group['lr']\n",
    "        print('learning_rate after epoch :{} is : {}'.format(epoch, cur_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8536a-afc0-4f31-aaa9-0715838080a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rtho(\"vgg\", \"cifar_10\", 10, 16, \"adam\", 0.0001, 0.9, 0, 0.0001, 1e-6, 100.0, False, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4f6af-c97b-4b39-a33c-36732d93878b",
   "metadata": {},
   "source": [
    "<img src='./figures/cifar10vgg_one_ACC-1.png' width=\"400\" height=\"400\">\n",
    "<img src='./figures/cifar10vgg_one_LOSS-1.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a2544-499e-4df5-909a-26e726f1a30b",
   "metadata": {},
   "source": [
    "#### This is what it should look like for the validation loss and validation accuracy of MARTHE and Hypergradient Descent with VGG and SGD on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc549e77-9403-4fec-94a3-8180d54f49d4",
   "metadata": {},
   "source": [
    "Use the graphs from the paper to compare HD with MARTHE. Dataset Cifar10, VGG, SGD. THis is what we are hoping from the students  \n",
    "What is MARTHE using/why is marthe better than hD? (Make it graduate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822479b-2324-4c10-b43a-519fdba2f8b2",
   "metadata": {},
   "source": [
    "## 4. Task for Graduate Students\n",
    "### a. Repeat task 1 and task 2, this time using ResNet and CIFAR100.\n",
    "<img src='./figures/cifar100resnet_one_ACC-1.png' width=\"400\" height=\"400\">\n",
    "<img src='./figures/cifar100resnet_one_LOSS-1.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88633397-ada8-473f-869b-1d7f514a7d0c",
   "metadata": {},
   "source": [
    "#### This is what it should look like for the validation loss and validation accuracy of MARTHE and Hypergradient Descent with Resnet and SGD on CIFAR100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61717d-7ac3-45ed-9881-343b166a3c67",
   "metadata": {},
   "source": [
    "### b. Students should also give an explanation about their findings: Read the MARTHE paper and provide an idea of why it is performing better than Hypergradient Descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
